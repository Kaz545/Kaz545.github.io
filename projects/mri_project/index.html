<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> BrainMRI-GPT | Mehlial Kazmi </title> <meta name="author" content="Mehlial Kazmi"> <meta name="description" content="Neuro-imaging analysis using Multimodal models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kaz545.github.io/projects/mri_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mehlial</span> Kazmi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">BrainMRI-GPT</h1> <p class="post-description">Neuro-imaging analysis using Multimodal models.</p> </header> <article> <p>Brain tumors significantly impact global mortality rates, making their detection and management a critical concern. Accurate segmentation of brain tumors from neuroimaging data plays a vital role in enhancing disease diagnosis, guiding treatment strategies, monitoring progression, and supporting clinical research. Effective segmentation is essential to identify both the tumor’s location and its size.</p> <p>We propose a novel approach to brain tumor detection that harnesses CLIP’s ability to align text descriptions with visual features. By fine-tuning this capability, we aim to identify and segment brain tumors with high accuracy, enabling clinicians to localize the tumor and assess its extent. To further enhance the usability of this system, we incorporate a Large Language Model (LLM) to facilitate interactive dialogues. This addition allows for natural language explanations of the model’s predictions, making the system more interpretable and suitable for clinical decision-making. Our approach not only seeks to advance diagnostic accuracy but also strives to bridge the gap between AI predictions and human understanding, fostering trust and collaboration in medical contexts.</p> <h3 id="data">Data</h3> <p>The visual data was taken from <a href="https://www.kaggle.com/datasets/ashkhagan/figshare-brain-tumor-dataset" rel="external nofollow noopener" target="_blank"> here </a>. This brain tumor dataset contains 3064 T1-weighted contrast-inhanced images from 233 patients with three kinds of brain tumor: meningioma (708 slices), glioma (1426 slices), and pituitary tumor (930 slices). As this dataset didnt contain the negative samples and we needed them to train our model so we took the negative samples, i.e samples with no brain tumor from here <a href="https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset" rel="external nofollow noopener" target="_blank"> here </a>. After augmentation the total number of samples in our dataset were 4659. We had two columns one corresponds to the MRI scan image and the second column was the binary tumor mask. For us the first column, the image was the input and the mask was something that we were trying to predict hence that was our target.</p> <h3 id="methodology">Methodology</h3> <p>The below image illustrates a high-level overview of our approach. We utilize both normal and abnormal textual descriptions, which are processed through the CLIP text encoder to obtain corresponding text embeddings. Simultaneously, normal and abnormal MRI images are passed through the CLIP visual encoder. From the visual encoder, we extract outputs from the 6th, 12th, 18th, and 24th hidden layers, representing intermediate patch-level features.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/e2e_flow_mri-480.webp 480w,/assets/img/e2e_flow_mri-800.webp 800w,/assets/img/e2e_flow_mri-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/e2e_flow_mri.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> E2E Flow. </div> <p>Since these visual features exist in a different embedding space than the encoded text, we align them using a trainable decoder. This decoder maps the visual features into the joint embedding space, ensuring compatibility with the text embeddings. The alignment is performed such that the dot product (or similarity measure) between the transformed visual features and text embeddings highlights the most relevant patches, producing precise tumor localization results. These localization maps, combined with carefully engineered textual prompts, are then passed to a large language model (LLM). This approach enhances the LLM’s understanding, enabling it to provide more accurate and contextually relevant responses about the tumor’s characteristics and location.</p> <p>In this study, we leverage the CLIP framework with the ViT-B/14 (Vision Transformer Base) model variant. Both the visual and textual encoders of CLIP are utilized: the visual encoder processes MRI image embeddings, while the textual encoder generates embeddings from associated textual descriptions. These multimodal embeddings serve as the foundation for tumor detection and localization.</p> <p>While image data for the visual encoder was readily available, corresponding textual data for the text encoder was absent. To address this, we systematically generated 20 text prompts for each category (normal and abnormal images). These prompts, identical across all images in their respective categories, provided descriptive textual input to the CLIP textual encoder.</p> <p>Moreover we utilize the prompt ensemble technique where we averaged the the text features extracted by the text encoder as the final text features for both normal and abnormal texts \(Ft ∈ R2×C\), where \(C\) denotes the number of channels of the feature. Let \(Fc ∈ R1×C\) represent the image features derived from the visual encoder for classification. The relative probabilities \(s\) of the MRI being classified as either normal or abnormal can then be expressed as:</p> \[s = \text{softmax}(F_c F_t^T)\] <p>We use the probability corresponding to the abnormal class as the anomaly score for the image.</p> <h4 id="decoder">Decoder</h4> <p>The CLIP model, originally designed for classification tasks, maps only the final image features to the joint embedding space, enabling direct comparison with text features. However, intermediate image features from earlier layers, which are crucial for fine-grained localization, are not inherently aligned with the joint embedding space. To address this limitation, we introduce a decoder that maps these intermediate image features into the joint embedding space, enabling meaningful comparisons with text embeddings. Using the transformer-based architecture ViT, we empirically divide the layers into four stages—specifically, the 6th, 12th, 18th, and 24th hidden layers. For each stage, the decoder processes the corresponding output features. The decoder is composed of a linear layer followed by a Leaky ReLU activation function, which ensures smooth handling of negative values. A final linear layer completes the mapping process, producing features aligned with the joint embedding space.</p> <p>This architecture as shown below effectively bridges the gap between intermediate visual features and textual embeddings, facilitating accurate patch-level similarity comparisons and improving localization performance. Furthermore, We employ the linear combination of Focal loss and Dice loss to train the decoder.</p> \[LOSS = DICE_LOSS + FOCAL_LOSS\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/decoder_mri-480.webp 480w,/assets/img/decoder_mri-800.webp 800w,/assets/img/decoder_mri-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/decoder_mri.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Decoder Structure. </div> <h4 id="lvlm">LvLM</h4> <p>After obtaining the tumor localization results, we passed them along with the corresponding textual prompts to a large language model (LLM) to extract detailed information from the MRI. We utilized Meta’s Llama 3.2 11B Vision Instruct Model, a powerful multimodal LLM optimized for visual tasks The Llama 3.2-Vision collection consists of pre-trained and instruction-tuned models that excel in image reasoning, visual recognition, captioning, and answering general questions about images.</p> <h3 id="experiments">Experiments</h3> <p>Training was done over 3 epochs, we utilized the Adam optimizer with specific hyperparameter settings: a learning rate of 0.01 and momentum parameters of 0.5 and 0.999. These values were selected to balance stability and convergence speed, especially given the inherent variability in the training data. The batch size was set to 16, ensuring efficient training while maintaining computational feasibility. On the large language model (LLM) side, we explored various prompting paradigms to evaluate their efficacy in different scenarios. These included zero-shot prompting, where we just passed the raw image and asked for the tumor location; instruction-based prompting, where we passed the localization results of the raw images and asked for the tumor location explicitly stating that red region indicates the tumor; and chain-of-thought (CoT) prompting, which encouraged step-by-step reasoning to solve more complex problems, here again we passed the localization results and just asked for the tumor location and details. These prompting strategies were assessed to determine their impact on the model’s ability to interpret and generate meaningful outputs.</p> <p>To evaluate our model’s localization performance, we relied on pixel-level metrics that are well-suited for assessing the precision and reliability of segmentation tasks. Specifically, the pixel-level AUROC score achieved an impressive 99.1, indicating the model’s ability to distinguish between tumor and non-tumor regions with high accuracy. Additionally, the accuracy score of 99.02 demonstrates that the majority of pixel classifications align with the ground truth, reflecting the robustness of the model in identifying tumor locations. However, the precision score of 73.22 highlights a slightly lower proportion of correctly identified tumor pixels among all predicted positives, suggesting some room for improvement in minimizing false positives.</p> <h3 id="results">Results</h3> <p>The below table presents the tumor localization results achieved by our approach. It clearly demonstrates the method’s effectiveness in accurately identifying and segmenting tumor regions. As evidenced by the visualized outcomes, the approach reliably highlights tumor locations with minimal deviations, underscoring its potential for precise tumor segmentation in neuro-imaging applications.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/results_mri-480.webp 480w,/assets/img/results_mri-800.webp 800w,/assets/img/results_mri-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/results_mri.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Qualitative Results. </div> <p>Moreover, the table below presents the responses generated by the LvLM using various prompting techniques. The zero-shot approach yielded suboptimal results, as the LvLM struggled to accurately identify the tumor location and provided incorrect details and answers. However, incorporating localization results in the prompt alongside instruct and CoT strategies significantly improved the LvLM’s performance. These methods enabled the model to correctly identify the tumor’s location and produce accurate responses.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lvlm_results-480.webp 480w,/assets/img/lvlm_results-800.webp 800w,/assets/img/lvlm_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/lvlm_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> LvLM Response. </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mehlial Kazmi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-brainmri-gpt",title:"BrainMRI-GPT",description:"Neuro-imaging analysis using Multimodal models.",section:"Projects",handler:()=>{window.location.href="/projects/mri_project/"}},{id:"projects-srm",title:"SRM",description:"Semantic-Based Recommendation Model.",section:"Projects",handler:()=>{window.location.href="/projects/srm_project/"}},{id:"projects-toxictrek",title:"ToxicTrek",description:"Detecting Social Media toxicity using Word2vec and Ensemble learning.",section:"Projects",handler:()=>{window.location.href="/projects/tt_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%65%68%6C%69%61%6C.%68%61%73%73%61%6E%39%39@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Kaz545","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/mehlial-seeking-data-science98","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>